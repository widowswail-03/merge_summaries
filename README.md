Strategy to implement the solution provided:
The code I have written assumes that the files submitted by the analysts is in a certain format. (each new news point is separated by blank lines). All reports will be uploaded to either a NOTION page or a cloud alternative where the python and javascript scripts are already uploaded and are being executed.
To solve this particular problem, I first decided to figure out how to figure out the issues of merging summaries by different analysts and how can this process be automated. For this, we start by using the python library spacy and the NLP model en_core_web_md. To avoid deduplication, I used the criteria of semantic similarity and anything having a similarity over 0.8 was considered to be a duplicate. To automate this process, I thought of running this python script daily at a fixed time either after the day ends or before the next day starts via the task scheduler on windows, or some other function depending on the cloud alternative being used. 
Next to understand how the summaries will be differentiated from one another, I have made 4 predefined categories based on the 2 sample summaries produced by ChatGPT. (NOTE: The number of subcategories and type of subcategories will change based on the requirements.) Based on these categories, I have mapped a few words that might be used by analysts in their summary and hence link those to the subcategories already created. In case the analysts have provided conflicting summaries, this will be flagged for review.
Further, for the webpage, I used the JS library React to build the UI. I also used the Fetch API to connect the frontend and backend. I used Tailwind CSS to style the dashboard. I have also ensured that there is dynamic tag filtering, which allows the user to figure out what subcategories the summaries are from today. Also, for conflicting summaries requiring action, there will be a conflict note that will be displayed.  
To execute this, the analysts must upload the files to the backend cloud server or Notion page. After this, assuming that the files have been formatted in the proper way and they are uploaded to the proper directory, the python script will be run automatically at a predefined time, for example 6:00 am, the next day. This the updates the webpage upon refreshing and the fund managers can easily access the summaries from the day before and take decisions based on that. To scale the solution, we can create a webpage/dashboard containing all the previous reports as well, but this will require a cloud-based system to implement. Also, it would require some additional changes in the javascript code written. Format for the summaries:
 
